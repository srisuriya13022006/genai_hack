{
  "doc_name": "math1",
  "page_count": 3,
  "pages": [
    "1  Introduction to ML and DS Formulas\n\nMachine learning (ML) and data science (DS) rely on mathematical foundations to model and analyze\ndata. This document presents key formulas used in these fields, formatted to span three pages.\n\n2  Linear Regression\n\nLinear regression models the relationship between a dependent variable y and independent variable(s) x\nusing a linear equation:\n                                       y = β0 + β1x + ϵ\n\nwhere:\n\n  • β0: Intercept.\n\n  • β1: Slope coeﬀicient.\n\n  • ϵ: Error term.\n\nThe least squares method minimizes the cost function:\n\n                                        1 Xn\n                                J(β0, β1) =       (yi −(β0 + β1xi))2\n                                 n\n                                                 i=1\n\nwhere n is the number of observations.\n   To fit the model, the normal equations are solved:\n\n                                 β = (XT X)−1XT y\n\nwhere X is the design matrix and y is the response vector.\n\n\n\n\n\n                                             1",
    "3  Logistic Regression\n\nLogistic regression predicts binary outcomes using the sigmoid function:\n\n                                                   1\n                              P(y = 1|x) =\n                                             1 + e−(β0+β1x)\n\nThe log-odds (logit) is:\n                                             p\n                                      logit(p) = ln      = β0 + β1x\n                                           1 −p\n\nThe cost function (log-loss) to optimize is:\n\n             Xn\n                         J(β) = −1     [yi ln(pi) + (1 −yi) ln(1 −pi)]\n                             n\n                                          i=1\n\n4  Gradient Descent\n\nGradient descent optimizes the cost function by iteratively adjusting parameters:\n\n                                               θj := θj −α∂J(θ)\n                                                     ∂θj\n\nwhere:\n\n  • θj: Parameter to update.\n\n  • α: Learning rate.\n\n  •  ∂J(θ) : Gradient of the cost function.        ∂θj\n\nFor linear regression, the gradient is:\n\n                             ∂J    2 Xn\n                        =      (hθ(xi) −yi)xi,j\n                                    ∂θj   n                                               i=1\n\nwhere hθ(x) = θT x is the hypothesis.\n\n\n\n\n\n                                             2",
    "5  Entropy and Information Gain\n\nEntropy measures uncertainty in a dataset:\n\n               Xn\n                             H(S) = −     pi log2(pi)\n                                                   i=1\n\nwhere pi is the probability of class i.\n   Information gain for a split is:\n\n                 X    |Sv|\n                            IG(S, A) = H(S) −            H(Sv)\n                                                                   |S|\n                                                       v∈Values(A)\n\nwhere Sv is the subset of S for attribute A value v.\n\n6  Additional ML/DS Notes\n\nCommon distance metrics include Euclidean distance:\n                    v\n                    u n                    u X\n                                       d(x, y) = t   (xi −yi)2\n                                                    i=1\n\nand cosine similarity:\n                                            x · y\n                                          cosine(x, y) =\n                                               ∥x∥∥y∥\n\nThese are widely used in clustering and recommendation systems.  For large datasets, regularization\n(e.g., L2: λ∥β∥2) prevents overfitting in regression models.\n\n\n\n\n\n                                             3"
  ]
}