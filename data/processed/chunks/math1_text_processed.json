{
  "doc_name": "math1",
  "page_count": 3,
  "chunks": [
    "1 Introduction to ML and DS Formulas Machine learning ( ML ) and data science ( DS ) rely on mathematical foundations to model and analyze data . This document presents key formulas used in these fields , formatted to span three pages . 2 Linear Regression Linear regression models the relationship between a dependent variable y and independent variable(s ) x using a linear equation : y = β0 + β1x + ϵ where : • β0 : Intercept . • β1 : Slope coeﬀicient . • ϵ : Error term . The least squares method minimizes the cost function : 1 Xn J(β0 , β1 ) = ( yi −(β0 + β1xi))2 n i=1 where n is the number of observations . To fit the model , the normal equations are solved : β = ( XT X)−1XT y where X is the design matrix and y is the response vector . 1",
    "3 Logistic Regression Logistic regression predicts binary outcomes using the sigmoid function : 1 P(y = 1|x ) = 1 + e−(β0+β1x ) The log - odds ( logit ) is : p logit(p ) = ln = β0 + β1x 1 −p The cost function ( log - loss ) to optimize is : Xn J(β ) = −1 [ yi ln(pi ) + ( 1 −yi ) ln(1 −pi ) ] n i=1 4 Gradient Descent Gradient descent optimizes the cost function by iteratively adjusting parameters : θj : = θj −α∂J(θ ) ∂θj where : • θj : Parameter to update . • α : Learning rate . • ∂J(θ ) : Gradient of the cost function . ∂θj For linear regression , the gradient is : ∂J 2 Xn = ( hθ(xi ) −yi)xi , j ∂θj n i=1 where hθ(x ) = θT x is the hypothesis . 2",
    "5 Entropy and Information Gain Entropy measures uncertainty in a dataset : Xn H(S ) = − pi log2(pi ) i=1 where pi is the probability of class i. Information gain for a split is : X |Sv| IG(S , A ) = H(S ) − H(Sv ) |S| v∈Values(A ) where Sv is the subset of S for attribute A value v. 6 Additional ML / DS Notes Common distance metrics include Euclidean distance : v u n u X d(x , y ) = t ( xi −yi)2 i=1 and cosine similarity : x · y cosine(x , y ) = ∥x∥∥y∥ These are widely used in clustering and recommendation systems . For large datasets , regularization ( e.g. , L2 : λ∥β∥2 ) prevents overfitting in regression models . 3"
  ]
}